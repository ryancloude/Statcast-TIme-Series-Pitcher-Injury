{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mixed Model Transformer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNBR2fTeU0p3q8iada1NWKu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dAOf4x1Gd5",
        "outputId": "b6f566eb-c7f0-49cf-a336-24b705641fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "keras.utils.set_random_seed(9)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "injury_history = pd.read_csv('/content/drive/MyDrive/data_490/injury_data/injury_history')\n",
        "bio_data = pd.read_csv('/content/drive/MyDrive/data_490/processed_data/bio_data.csv')\n",
        "ts_data = pd.read_pickle('/content/drive/MyDrive/data_490/processed_data/thirty_day_timestep_df')"
      ],
      "metadata": {
        "id": "i9xPCCfc1WNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_data.player_age = bio_data['player_age'].str.split(' ').str[0]"
      ],
      "metadata": {
        "id": "fRYrSMxaRFTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = np.random.permutation(len(ts_data))\n",
        "train_size = int(len(indexes)*.8)\n",
        "valid_size = int(train_size*.2)\n",
        "train_indexes = indexes[valid_size:train_size]\n",
        "valid_indexes = indexes[:valid_size]\n",
        "test_indexes = indexes[train_size:]"
      ],
      "metadata": {
        "id": "lh1sdJDT4_-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_data = bio_data.fillna(-1)\n",
        "injury_history = injury_history.fillna(-1)"
      ],
      "metadata": {
        "id": "M8Cf9O5wJSux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_train = np.stack(ts_data.loc[train_indexes, 'time_series'].values).astype(np.float16)[:,:,:-1]\n",
        "ts_valid = np.stack(ts_data.loc[valid_indexes, 'time_series'].values).astype(np.float16)[:,:,:-1]\n",
        "ts_test = np.stack(ts_data.loc[test_indexes, 'time_series'].values).astype(np.float16)[:,:,:-1]\n",
        "bio_train = bio_data.iloc[train_indexes, 2:].astype(np.float16)\n",
        "bio_valid = bio_data.iloc[valid_indexes, 2:].astype(np.float16)\n",
        "bio_test = bio_data.iloc[test_indexes, 2:].astype(np.float16)\n",
        "injury_train = injury_history.iloc[train_indexes, 2:].astype(np.float16)\n",
        "injury_valid = injury_history.iloc[valid_indexes, 2:].astype(np.float16)\n",
        "injury_test = injury_history.iloc[test_indexes, 2:].astype(np.float16)\n",
        "train_target =  ts_data.loc[train_indexes, 'injured'].astype(np.float16)\n",
        "valid_target =  ts_data.loc[valid_indexes, 'injured'].astype(np.float16)\n",
        "test_target = ts_data.loc[test_indexes, 'injured'].astype(np.float16)"
      ],
      "metadata": {
        "id": "jmtR3SYv57Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    \"\"\"Positional encoding.\"\"\"\n",
        "    def __init__(self, dropout=0, time_steps=30, features=85):\n",
        "        super().__init__()\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        p = np.zeros(np.shape((1, time_steps, features)))\n",
        "        columns = np.arange(features)\n",
        "        rows = np.arange(time_steps)\n",
        "        p = rows.reshape(-1,1)/1000**(columns*2/features)\n",
        "        p[:, 0::2] = np.sin(p[:,0::2])\n",
        "        p[:, 1::2] = np.cos(p[:, 1::2])\n",
        "        self.p = p\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        x = x + self.p\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "1hRm3p_4oERM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "def transformer_encoder(inputs, head_size, num_heads, conv_filters, kernel_size, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = keras.layers.Normalization()(inputs)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=conv_filters, kernel_size=kernel_size, activation=\"relu\", padding='same')(x)\n",
        "    X = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "znvTGaxs-SZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mixed_model(\n",
        "    ts_input_shape,\n",
        "    injury_input_shape,\n",
        "    bio_input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    conv_filters,\n",
        "    kernel_size,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    ts_input = keras.Input(shape=ts_input_shape)\n",
        "    x = ts_input\n",
        "\n",
        "    #Embedding\n",
        "    x = layers.Flatten(input_shape=ts_input_shape)(x)\n",
        "    x = layers.Dense(ts_input_shape[-1]*ts_input_shape[-2], activation='tanh')(x)\n",
        "    x = layers.Reshape(ts_input_shape)(x)\n",
        "\n",
        "    #Positional Encoding\n",
        "    x = PositionalEncoding()(x)\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, conv_filters, kernel_size, dropout)\n",
        "\n",
        "\n",
        "    bio_input = keras.Input(shape=bio_input_shape)\n",
        "    bio = layers.experimental.preprocessing.Normalization()(bio_input)\n",
        "\n",
        "    injury_input = keras.Input(shape=injury_input_shape) \n",
        "    injury = layers.experimental.preprocessing.Normalization()(injury_input)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, injury, bio])\n",
        "\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    inputs = [ts_input, injury_input, bio_input]\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "O8XU07y3-TKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_shape = np.shape(ts_train)[1:]\n",
        "injury_shape = np.shape(injury_train)[1:]\n",
        "bio_shape = np.shape(bio_train)[1:]\n",
        "\n",
        "\n",
        "mixed_model = build_mixed_model(\n",
        "    ts_shape,\n",
        "    injury_shape,\n",
        "    bio_shape,\n",
        "    head_size=256,\n",
        "    num_heads=8,\n",
        "    conv_filters=64,\n",
        "    kernel_size=3,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[64,64,64,64],\n",
        "    mlp_dropout=0.2,\n",
        "    dropout=0.2,\n",
        ")\n",
        "\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=1e-6)\n",
        "opt = tfa.optimizers.SWA(opt)\n",
        "\n",
        "mixed_model.compile(\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=opt,\n",
        "    metrics=[keras.metrics.AUC()],\n",
        "    steps_per_execution=32,\n",
        "    jit_compile=True\n",
        ")"
      ],
      "metadata": {
        "id": "rtWrOfyRZNCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr, warmup_epochs=20, decay_epochs=100, initial_lr=1e-8, base_lr=1e-4, min_lr=1e-7):\n",
        "    if epoch <= warmup_epochs:\n",
        "        pct = epoch / warmup_epochs\n",
        "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
        "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
        "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
        "        return ((base_lr - min_lr) * pct) + min_lr\n",
        "\n",
        "    return min_lr"
      ],
      "metadata": {
        "id": "548GY74RrM7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.keras.utils.plot_model(mixed_model)"
      ],
      "metadata": {
        "id": "5KhIJX6IZOQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True),\n",
        "             tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
        "\n",
        "mixed_model.fit(\n",
        "    [ts_train, injury_train, bio_train],\n",
        "    train_target,\n",
        "    validation_data=[[ts_valid, injury_valid, bio_valid], valid_target],\n",
        "    epochs=300,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "#model.evaluate(ts_test, test_target, verbose=1)"
      ],
      "metadata": {
        "id": "jjTN3uSd-Xp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d4078d-3b50-499b-844d-ad28267802de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "662/662 [==============================] - 82s 124ms/step - loss: 964.3180 - auc: 0.5009 - val_loss: 634.3347 - val_auc: 0.5027 - lr: 1.0000e-08\n",
            "Epoch 2/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 549.2436 - auc: 0.5052 - val_loss: 6.6718 - val_auc: 0.5247 - lr: 5.0095e-06\n",
            "Epoch 3/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 107.1518 - auc: 0.5037 - val_loss: 15.5604 - val_auc: 0.4999 - lr: 1.0009e-05\n",
            "Epoch 4/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 31.3796 - auc: 0.5113 - val_loss: 20.8807 - val_auc: 0.5000 - lr: 1.5008e-05\n",
            "Epoch 5/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 22.9755 - auc: 0.5004 - val_loss: 20.8080 - val_auc: 0.5000 - lr: 2.0008e-05\n",
            "Epoch 6/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 19.4441 - auc: 0.5004 - val_loss: 16.9998 - val_auc: 0.5000 - lr: 2.5008e-05\n",
            "Epoch 7/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 14.9804 - auc: 0.4991 - val_loss: 10.1857 - val_auc: 0.5000 - lr: 3.0007e-05\n",
            "Epoch 8/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 10.0249 - auc: 0.4998 - val_loss: 4.9239 - val_auc: 0.5000 - lr: 3.5006e-05\n",
            "Epoch 9/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 6.8646 - auc: 0.5036 - val_loss: 2.9107 - val_auc: 0.5000 - lr: 4.0006e-05\n",
            "Epoch 10/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 4.7556 - auc: 0.4988 - val_loss: 1.9280 - val_auc: 0.5000 - lr: 4.5006e-05\n",
            "Epoch 11/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 3.3531 - auc: 0.5024 - val_loss: 1.3316 - val_auc: 0.5000 - lr: 5.0005e-05\n",
            "Epoch 12/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 2.3850 - auc: 0.4978 - val_loss: 0.8394 - val_auc: 0.5000 - lr: 5.5005e-05\n",
            "Epoch 13/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 1.6262 - auc: 0.4992 - val_loss: 0.5203 - val_auc: 0.4992 - lr: 6.0004e-05\n",
            "Epoch 14/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 1.1395 - auc: 0.4929 - val_loss: 0.3132 - val_auc: 0.5131 - lr: 6.5003e-05\n",
            "Epoch 15/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.8084 - auc: 0.4970 - val_loss: 0.2437 - val_auc: 0.5121 - lr: 7.0003e-05\n",
            "Epoch 16/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.5756 - auc: 0.4841 - val_loss: 0.2059 - val_auc: 0.3704 - lr: 7.5002e-05\n",
            "Epoch 17/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.4265 - auc: 0.4538 - val_loss: 0.1580 - val_auc: 0.4160 - lr: 8.0002e-05\n",
            "Epoch 18/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.3128 - auc: 0.4484 - val_loss: 0.1308 - val_auc: 0.4709 - lr: 8.5001e-05\n",
            "Epoch 19/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.2664 - auc: 0.4523 - val_loss: 0.1231 - val_auc: 0.4679 - lr: 9.0001e-05\n",
            "Epoch 20/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.2218 - auc: 0.4685 - val_loss: 0.1107 - val_auc: 0.5157 - lr: 9.5000e-05\n",
            "Epoch 21/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1817 - auc: 0.4820 - val_loss: 0.1091 - val_auc: 0.5017 - lr: 1.0000e-04\n",
            "Epoch 22/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1653 - auc: 0.4846 - val_loss: 0.0987 - val_auc: 0.5874 - lr: 9.9001e-05\n",
            "Epoch 23/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1436 - auc: 0.4987 - val_loss: 0.0941 - val_auc: 0.5960 - lr: 9.8002e-05\n",
            "Epoch 24/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1383 - auc: 0.5280 - val_loss: 0.1001 - val_auc: 0.6137 - lr: 9.7003e-05\n",
            "Epoch 25/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1296 - auc: 0.5488 - val_loss: 0.1144 - val_auc: 0.5736 - lr: 9.6004e-05\n",
            "Epoch 26/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1182 - auc: 0.5435 - val_loss: 0.1075 - val_auc: 0.5866 - lr: 9.5005e-05\n",
            "Epoch 27/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1127 - auc: 0.5652 - val_loss: 0.1091 - val_auc: 0.5538 - lr: 9.4006e-05\n",
            "Epoch 28/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1152 - auc: 0.5602 - val_loss: 0.1163 - val_auc: 0.5665 - lr: 9.3007e-05\n",
            "Epoch 29/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1117 - auc: 0.5559 - val_loss: 0.1009 - val_auc: 0.5709 - lr: 9.2008e-05\n",
            "Epoch 30/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.1013 - auc: 0.5636 - val_loss: 0.0999 - val_auc: 0.6025 - lr: 9.1009e-05\n",
            "Epoch 31/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0990 - auc: 0.5808 - val_loss: 0.1000 - val_auc: 0.5941 - lr: 9.0010e-05\n",
            "Epoch 32/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0989 - auc: 0.5908 - val_loss: 0.0932 - val_auc: 0.6022 - lr: 8.9011e-05\n",
            "Epoch 33/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0957 - auc: 0.5842 - val_loss: 0.0934 - val_auc: 0.6106 - lr: 8.8012e-05\n",
            "Epoch 34/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0952 - auc: 0.5955 - val_loss: 0.0976 - val_auc: 0.6320 - lr: 8.7013e-05\n",
            "Epoch 35/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0914 - auc: 0.5972 - val_loss: 0.0913 - val_auc: 0.6384 - lr: 8.6014e-05\n",
            "Epoch 36/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0896 - auc: 0.6098 - val_loss: 0.0910 - val_auc: 0.6456 - lr: 8.5015e-05\n",
            "Epoch 37/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0889 - auc: 0.6075 - val_loss: 0.0911 - val_auc: 0.6588 - lr: 8.4016e-05\n",
            "Epoch 38/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0890 - auc: 0.6039 - val_loss: 0.0889 - val_auc: 0.6443 - lr: 8.3017e-05\n",
            "Epoch 39/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0854 - auc: 0.6348 - val_loss: 0.0918 - val_auc: 0.6571 - lr: 8.2018e-05\n",
            "Epoch 40/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0864 - auc: 0.6284 - val_loss: 0.0896 - val_auc: 0.6423 - lr: 8.1019e-05\n",
            "Epoch 41/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0862 - auc: 0.6246 - val_loss: 0.0895 - val_auc: 0.6450 - lr: 8.0020e-05\n",
            "Epoch 42/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0853 - auc: 0.6313 - val_loss: 0.0917 - val_auc: 0.6500 - lr: 7.9021e-05\n",
            "Epoch 43/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0844 - auc: 0.6333 - val_loss: 0.0917 - val_auc: 0.6543 - lr: 7.8022e-05\n",
            "Epoch 44/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0839 - auc: 0.6399 - val_loss: 0.0927 - val_auc: 0.6365 - lr: 7.7023e-05\n",
            "Epoch 45/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0853 - auc: 0.6275 - val_loss: 0.0904 - val_auc: 0.6499 - lr: 7.6024e-05\n",
            "Epoch 46/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0847 - auc: 0.6325 - val_loss: 0.0962 - val_auc: 0.6509 - lr: 7.5025e-05\n",
            "Epoch 47/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0838 - auc: 0.6402 - val_loss: 0.0928 - val_auc: 0.6415 - lr: 7.4026e-05\n",
            "Epoch 48/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0830 - auc: 0.6448 - val_loss: 0.0929 - val_auc: 0.6482 - lr: 7.3027e-05\n",
            "Epoch 49/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0835 - auc: 0.6478 - val_loss: 0.0905 - val_auc: 0.6485 - lr: 7.2028e-05\n",
            "Epoch 50/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0821 - auc: 0.6589 - val_loss: 0.0894 - val_auc: 0.6607 - lr: 7.1029e-05\n",
            "Epoch 51/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0818 - auc: 0.6533 - val_loss: 0.0947 - val_auc: 0.6524 - lr: 7.0030e-05\n",
            "Epoch 52/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0821 - auc: 0.6574 - val_loss: 0.0896 - val_auc: 0.6580 - lr: 6.9031e-05\n",
            "Epoch 53/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0821 - auc: 0.6555 - val_loss: 0.0914 - val_auc: 0.6533 - lr: 6.8032e-05\n",
            "Epoch 54/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0807 - auc: 0.6684 - val_loss: 0.0898 - val_auc: 0.6525 - lr: 6.7033e-05\n",
            "Epoch 55/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0812 - auc: 0.6576 - val_loss: 0.0948 - val_auc: 0.6503 - lr: 6.6034e-05\n",
            "Epoch 56/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0813 - auc: 0.6508 - val_loss: 0.0915 - val_auc: 0.6573 - lr: 6.5035e-05\n",
            "Epoch 57/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0815 - auc: 0.6363 - val_loss: 0.0920 - val_auc: 0.6507 - lr: 6.4036e-05\n",
            "Epoch 58/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0809 - auc: 0.6458 - val_loss: 0.0950 - val_auc: 0.6564 - lr: 6.3037e-05\n",
            "Epoch 59/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0804 - auc: 0.6527 - val_loss: 0.0982 - val_auc: 0.6611 - lr: 6.2038e-05\n",
            "Epoch 60/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0807 - auc: 0.6513 - val_loss: 0.0949 - val_auc: 0.6605 - lr: 6.1039e-05\n",
            "Epoch 61/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0801 - auc: 0.6564 - val_loss: 0.0949 - val_auc: 0.6578 - lr: 6.0040e-05\n",
            "Epoch 62/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0802 - auc: 0.6586 - val_loss: 0.0889 - val_auc: 0.6560 - lr: 5.9041e-05\n",
            "Epoch 63/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0800 - auc: 0.6567 - val_loss: 0.0953 - val_auc: 0.6701 - lr: 5.8042e-05\n",
            "Epoch 64/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0794 - auc: 0.6720 - val_loss: 0.0933 - val_auc: 0.6655 - lr: 5.7043e-05\n",
            "Epoch 65/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0796 - auc: 0.6673 - val_loss: 0.0946 - val_auc: 0.6733 - lr: 5.6044e-05\n",
            "Epoch 66/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0798 - auc: 0.6655 - val_loss: 0.0947 - val_auc: 0.6589 - lr: 5.5045e-05\n",
            "Epoch 67/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0792 - auc: 0.6748 - val_loss: 0.0926 - val_auc: 0.6655 - lr: 5.4046e-05\n",
            "Epoch 68/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0796 - auc: 0.6667 - val_loss: 0.0929 - val_auc: 0.6727 - lr: 5.3047e-05\n",
            "Epoch 69/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0795 - auc: 0.6725 - val_loss: 0.0981 - val_auc: 0.6709 - lr: 5.2048e-05\n",
            "Epoch 70/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0787 - auc: 0.6877 - val_loss: 0.0915 - val_auc: 0.6742 - lr: 5.1049e-05\n",
            "Epoch 71/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0802 - auc: 0.6459 - val_loss: 0.0961 - val_auc: 0.6762 - lr: 5.0050e-05\n",
            "Epoch 72/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0792 - auc: 0.6724 - val_loss: 0.0949 - val_auc: 0.6732 - lr: 4.9051e-05\n",
            "Epoch 73/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0789 - auc: 0.6785 - val_loss: 0.0954 - val_auc: 0.6686 - lr: 4.8052e-05\n",
            "Epoch 74/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0794 - auc: 0.6793 - val_loss: 0.0940 - val_auc: 0.6676 - lr: 4.7053e-05\n",
            "Epoch 75/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0782 - auc: 0.6929 - val_loss: 0.0981 - val_auc: 0.6635 - lr: 4.6054e-05\n",
            "Epoch 76/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0788 - auc: 0.6861 - val_loss: 0.0944 - val_auc: 0.6590 - lr: 4.5055e-05\n",
            "Epoch 77/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0787 - auc: 0.6861 - val_loss: 0.0909 - val_auc: 0.6681 - lr: 4.4056e-05\n",
            "Epoch 78/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0787 - auc: 0.6837 - val_loss: 0.0931 - val_auc: 0.6675 - lr: 4.3057e-05\n",
            "Epoch 79/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0785 - auc: 0.6871 - val_loss: 0.0970 - val_auc: 0.6611 - lr: 4.2058e-05\n",
            "Epoch 80/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0783 - auc: 0.6881 - val_loss: 0.0949 - val_auc: 0.6738 - lr: 4.1059e-05\n",
            "Epoch 81/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0777 - auc: 0.7020 - val_loss: 0.0964 - val_auc: 0.6600 - lr: 4.0060e-05\n",
            "Epoch 82/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0787 - auc: 0.6844 - val_loss: 0.1002 - val_auc: 0.6526 - lr: 3.9061e-05\n",
            "Epoch 83/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0779 - auc: 0.6970 - val_loss: 0.1030 - val_auc: 0.6435 - lr: 3.8062e-05\n",
            "Epoch 84/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0779 - auc: 0.6969 - val_loss: 0.0932 - val_auc: 0.6681 - lr: 3.7063e-05\n",
            "Epoch 85/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0786 - auc: 0.6920 - val_loss: 0.0965 - val_auc: 0.6558 - lr: 3.6064e-05\n",
            "Epoch 86/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0778 - auc: 0.6998 - val_loss: 0.1000 - val_auc: 0.6577 - lr: 3.5065e-05\n",
            "Epoch 87/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0780 - auc: 0.6956 - val_loss: 0.0954 - val_auc: 0.6646 - lr: 3.4066e-05\n",
            "Epoch 88/300\n",
            "662/662 [==============================] - 30s 45ms/step - loss: 0.0771 - auc: 0.7095 - val_loss: 0.0907 - val_auc: 0.6674 - lr: 3.3067e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe8098da3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = mixed_model.predict([ts_test, injury_test, bio_test])"
      ],
      "metadata": {
        "id": "prfNl9R1RSn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk = np.where(pred > np.percentile(pred, 95))\n",
        "normal_risk = np.where((pred > np.percentile(pred, 40)) & (pred < np.percentile(pred, 95)))\n",
        "low_risk = np.where(pred < np.percentile(pred, 40))"
      ],
      "metadata": {
        "id": "qIrbvxCBhbJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(test_target.iloc[normal_risk[0]])"
      ],
      "metadata": {
        "id": "OmKevy0Jh82W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6c4a13-248e-4109-b509-e24c5517c9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0202"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(test_target.iloc[low_risk[0]])"
      ],
      "metadata": {
        "id": "4yqnhEkwOcW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f6c1e0-8e2d-44b6-a508-aab4931ea90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00898"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(test_target.iloc[high_risk[0]])"
      ],
      "metadata": {
        "id": "bWtn0VEET2nw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaa73ff-8e5e-4878-cfd2-4d65a10b57a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0461"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(pred, 50)"
      ],
      "metadata": {
        "id": "CLbq60MqPGWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(pred)"
      ],
      "metadata": {
        "id": "tLhPKuF7vmbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "injured = np.where(test_target == 1)\n",
        "healthy = np.where(test_target == 0)"
      ],
      "metadata": {
        "id": "Mxx8TBuYgpKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(pred)"
      ],
      "metadata": {
        "id": "FtrdksrdqKgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(pred[injured[0]])"
      ],
      "metadata": {
        "id": "bM_9INDCg1Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(pred[healthy[0]])"
      ],
      "metadata": {
        "id": "IjBIVS15g_rY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}